!pip install telethon tqdm nest_asyncio ffmpeg-python
!apt-get install ffmpeg -y  # Required for Colab to process video

import os
import asyncio
import math
import random
import ffmpeg
import zipfile
import subprocess
import requests
import mimetypes  # Added to handle missing extensions
!pip install py7zr
import shutil
#import re
!pip install Pillow
#!pip uninstall -y tgcrypto
#!pip install tgcrypto
#!pip install pyaesni
#!pip install tgcrypto
import hashlib
import inspect
import logging
#import os
from telethon import TelegramClient, functions, types
from tqdm.notebook import tqdm
from urllib.parse import unquote
import nest_asyncio


# --- CONFIGURATION ---
API_ID = 2523662
API_HASH = '7351d84a540f599b95f71ca9d76bee96'
FILE_PATH = 'https://arcjav.arcjavipzz.workers.dev/0:/051-100/%E5%A6%83%E6%9C%88%E7%95%99%E8%A1%A3/ATID-321/ATID-321%20%E7%A6%81%E6%96%AD%E4%BA%8C%E7%A9%B4%E3%81%AE%E3%82%A2%E3%83%8A%E3%83%AB%E6%82%A6%E6%A5%BD%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88.mp4'
RECEIVER = '@tomsend_bot'

# OPTIONS
EXTRACT_ARCHIVE = True
ARCHIVE_PASSWORD = 'ABC'
SPLIT_THRESHOLD = 2000 * 1024 * 1024
#SPLIT_THRESHOLD = 1000 * 512 * 512
UPLOAD_AS_VIDEO = True
AUTO_THUMBNAIL = True

# --- FIXED DOWNLOAD FUNCTION ---

def download_file(url):
    print(f"üì• Connecting to: {url}")
    with requests.get(url, stream=True) as r:
        r.raise_for_status()

        # 1. Try to get filename from headers
        content_disposition = r.headers.get('Content-Disposition')
        if content_disposition and 'filename=' in content_disposition:
            local_filename = content_disposition.split('filename=')[-1].strip('"')
        else:
        # 2. Fallback to URL path
            local_filename = url.split('/')[-1].split('?')[0]

        # --- NEW: Decode URL-encoded characters (e.g., %20 to space) ---
        local_filename = unquote(local_filename)

        # 3. Fix missing extension using Content-Type
        if '.' not in local_filename:
            content_type = r.headers.get('Content-Type')
            extension = mimetypes.guess_extension(content_type.split(';')[0])
            if extension:
                local_filename += extension

        print(f"üì¶ Saving as: {local_filename}")

        total_size = int(r.headers.get('content-length', 0))
        with open(local_filename, 'wb') as f, tqdm(
            total=total_size, unit='B', unit_scale=True, desc="Downloading"
        ) as pbar:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
                pbar.update(len(chunk))

    return local_filename

# --- HELPER FUNCTIONS ---

def get_video_metadata(file_path):
    try:
        probe = ffmpeg.probe(file_path)
        video_stream = next((s for s in probe['streams'] if s['codec_type'] == 'video'), None)
        duration = float(video_stream.get('duration', probe['format'].get('duration', 0)))
        width = int(video_stream.get('width', 1280))
        height = int(video_stream.get('height', 720))
        return duration, width, height
    except Exception:
        return 0, 1280, 720

def generate_thumbnail_at_time(video_path, timestamp):
    thumb_path = f"thumb_{random.getrandbits(16)}.jpg"
    try:
        (ffmpeg.input(video_path, ss=timestamp).filter('scale', 320, -1)
         .output(thumb_path, vframes=1).overwrite_output().run(capture_stdout=True, capture_stderr=True))
        return thumb_path
    except: return None

def extract_archive_fast(file_path, password=None):
    extract_to = "extracted_files"
    if os.path.exists(extract_to): shutil.rmtree(extract_to)
    os.makedirs(extract_to)
    print(f"üì¶ Extracting File with System 7z (High Speed Mode)...")
    pass_arg = f'-p"{password}"' if password else ""
    cmd = f'7z x "{file_path}" {pass_arg} -o"{extract_to}" -y'
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    process.communicate()
    return extract_to

def split_file(file_path):
    file_size = os.path.getsize(file_path)
    if file_size <= SPLIT_THRESHOLD:
        return [file_path]
  # print(f"‚úÇÔ∏è Splitting: {os.path.basename(file_path)}")
    print(f"‚úÇÔ∏è File is too large : {os.path.basename(file_path)}({file_size / (1024**3):.2f} GB). Splitting File...")
    base_name, extension = os.path.splitext(file_path)
    chunks = []
    part_num = 1
    with open(file_path, 'rb') as f:
        while True:
            chunk_data = f.read(SPLIT_THRESHOLD)
            if not chunk_data: break
            chunk_name = f"{base_name}.part{part_num}{extension}"
            with open(chunk_name, 'wb') as chunk_file:
                chunk_file.write(chunk_data)
            chunks.append(chunk_name)
            part_num += 1
    return chunks


async def fast_upload_file(client, file_path, pbar):
    file_size = os.path.getsize(file_path)
    # 512KB is the maximum part size for Telegram
    part_size = 512 * 1024 
    file_id = random.getrandbits(63)
    parts_count = math.ceil(file_size / part_size)
    
    # The queue holds the chunks to be uploaded
    queue = asyncio.Queue(maxsize=20) 
    # Number of simultaneous upload workers (12-16 is best for Colab)
    pool_size = 12 

    async def worker():
        while True:
            item = await queue.get()
            if item is None:
                break
            idx, chunk = item
            try:
                # This is the actual upload call to Telegram
                await client(functions.upload.SaveBigFilePartRequest(
                    file_id=file_id, 
                    file_part=idx, 
                    file_total_parts=parts_count, 
                    bytes=chunk
                ))
                pbar.update(len(chunk))
            except Exception as e:
                print(f"Error uploading part {idx}: {e}")
            finally:
                queue.task_done()

    # 1. Create the worker pool
    workers = [asyncio.create_task(worker()) for _ in range(pool_size)]

    # 2. Feed chunks into the queue
    with open(file_path, 'rb') as f:
        for i in range(parts_count):
            chunk_data = f.read(part_size)
            await queue.put((i, chunk_data))
    # 3. Add "None" signals to tell workers to shut down
    for _ in range(pool_size):
        await queue.put(None)
    # 4. Wait for all workers to finish
    await asyncio.gather(*workers)
    return types.InputFileBig(id=file_id, parts=parts_count, name=os.path.basename(file_path))

def generate_preview_images(video_path, duration, num_images=6):
    """Generates a list of preview images from the video."""
    preview_paths = []
    if duration <= 0:
        return preview_paths

    # Calculate timestamps (avoiding the very beginning and very end)
    interval = duration / (num_images + 1)
    timestamps = [interval * (i + 1) for i in range(num_images)]

    for i, ts in enumerate(timestamps):
        thumb_path = f"preview_{i}_{random.getrandbits(16)}.jpg"
        try:
            (ffmpeg.input(video_path, ss=ts)
             .filter('scale', 640, -1) # Slightly higher res for previews
             .output(thumb_path, vframes=1)
             .overwrite_output()
             .run(capture_stdout=True, capture_stderr=True))
            if os.path.exists(thumb_path):
                preview_paths.append(thumb_path)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to generate preview at {ts}s: {e}")
            
    return preview_paths

async def process_and_upload(client, file_path):
    duration, width, height = get_video_metadata(file_path)
    file_list = split_file(file_path)
    total_parts = len(file_list)
    time_step = duration / total_parts if total_parts > 0 else 0

    # Determine if it's a video once
    file_ext = file_path.lower()
    is_video = UPLOAD_AS_VIDEO and file_ext.endswith(('.mp4', '.mkv', '.wmv', '.flv', '.mov', '.avi', '.ts'))
    is_image = file_ext.endswith(('.jpg', '.jpeg', '.png', '.webp'))

    for i, f_path in enumerate(file_list):
        current_thumb = None
        
        # Generate single thumbnail for the video file icon
        if is_video and AUTO_THUMBNAIL:
            timestamp = (i * time_step) + min(5, time_step / 2)
            # Reusing the simple logic for the main file thumb
            current_thumb = f"thumb_main_{random.getrandbits(16)}.jpg"
            try:
                (ffmpeg.input(file_path, ss=timestamp).filter('scale', 320, -1)
                 .output(current_thumb, vframes=1).overwrite_output().run(capture_stdout=True, capture_stderr=True))
            except: current_thumb = None

        current_caption = os.path.basename(f_path)
        file_size = os.path.getsize(f_path)

        with tqdm(total=file_size, unit='B', unit_scale=True, desc=f"üöÄ Uploading {current_caption}") as pbar:
            uploaded_file = await fast_upload_file(client, f_path, pbar)

            try:
                await client.send_file(
                    RECEIVER,
                    uploaded_file,
                    caption=f"üìÇ **{current_caption}**",
                    thumb=current_thumb,
                    force_document=not (is_video or is_image),
                    attributes=[types.DocumentAttributeVideo(
                        duration=int(duration / total_parts), w=width, h=height, supports_streaming=True
                    )] if is_video else [],
                    parse_mode='markdown'
                )
            except Exception as e:
                await client.send_file(
                    RECEIVER,
                    f_path,
                    caption=f"üìÇ **{current_caption}** ",
                    force_document=False,
                    parse_mode='markdown'
                )

        # --- PREVIEW ALBUM LOGIC ---
        # Generate and send 6-image album only for the first part of a video
        if is_video and i == 0:
            print(f"üñºÔ∏è Generating 6-image preview album for {current_caption}...")
            previews = generate_preview_images(file_path, duration, 6)
            if previews:
                try:
                    # Send all previews as a single album (media group)
                    await client.send_file(RECEIVER, previews, caption=f"üñºÔ∏è Preview: {os.path.basename(file_path)}")
                finally:
                    # Cleanup previews
                    for p in previews:
                        if os.path.exists(p): os.remove(p)

        # Cleanup video thumbnail and chunks
        if current_thumb and os.path.exists(current_thumb):
            os.remove(current_thumb)
        if f_path != file_path and os.path.exists(f_path):
            os.remove(f_path)


async def main():
    nest_asyncio.apply()

    # Determine if we are dealing with a URL, a single file, or a directory
    is_url = FILE_PATH.startswith(('http://', 'https://'))

    async with TelegramClient('colab_turbo_session', API_ID, API_HASH) as client:
        temp_dir = "extracted_files"
        files_to_upload = []

        if is_url:
            # Handle Single URL Download
            target_file = download_file(FILE_PATH)
            files_to_upload.append(target_file)

        elif os.path.isdir(FILE_PATH):
            # NEW: Handle Directory Batch with Alphabetical Sorting
            print(f"üìÇ Scanning folder: {FILE_PATH}")
            for root, dirs, files in os.walk(FILE_PATH):
                # Sort directories and files in-place so os.walk follows alphabetical order
                dirs.sort() 
                files.sort()
                for f in files:
                    # Skip hidden files and system files
                    if not f.startswith('.'):
                        files_to_upload.append(os.path.join(root, f))

        elif os.path.exists(FILE_PATH):
            # Handle Single Local File
            files_to_upload.append(FILE_PATH)

        else:
            print(f"‚ùå Error: {FILE_PATH} is not a valid URL, file, or directory.")
            return

        # Process the list of files
        for file in files_to_upload:
            # Handle extraction if the file is an archive
            if EXTRACT_ARCHIVE and file.lower().endswith(('.zip', '.7z')):
                extracted_dir = extract_archive_fast(file, ARCHIVE_PASSWORD)  
                # --- SORTED EXTRACTION SCAN ---
                for root, dirs, e_files in os.walk(extracted_dir):
                    # Sort directories and files in-place for alphabetical delivery
                    dirs.sort()
                    e_files.sort()
                    
                    for ef in e_files:
                        if not ef.startswith('.'):
                            await process_and_upload(client, os.path.join(root, ef))
                # ------------------------------
                shutil.rmtree(extracted_dir)
            else:
                # Direct upload for non-archives or if extraction is disabled
                await process_and_upload(client, file)
            # Cleanup: If it was a downloaded URL, delete the downloaded file
            if is_url and os.path.exists(file):
                os.remove(file)

        print("\n‚úÖ All finished!")

await main()